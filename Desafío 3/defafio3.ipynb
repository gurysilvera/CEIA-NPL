{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd2a08e5",
   "metadata": {},
   "source": [
    "# CEIA - Procesamiento de Lenguaje Natural\n",
    "\n",
    "## Estudiante: a2124 - Ricardo Silvera\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bab04a6",
   "metadata": {},
   "source": [
    "# Desafío 3\n",
    "\n",
    "### Consigna\n",
    "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
    "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
    "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
    "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
    "\n",
    "\n",
    "### Sugerencias\n",
    "- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n",
    "- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n",
    "- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec0c8d",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa901616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 11:41:14.150716: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-12-07 11:41:14.175513: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-12-07 11:41:14.175550: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-07 11:41:14.191894: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-07 11:41:15.168560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-12-07 11:41:15.168560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONFIGURACIÓN DE GPU OPTIMIZADA\n",
      "======================================================================\n",
      "\n",
      "TensorFlow version: 2.16.2\n",
      "¿Construido con CUDA?: True\n",
      "\n",
      "--- Dispositivos disponibles ---\n",
      "CPUs: 1\n",
      "GPUs: 1\n",
      "\n",
      "✓ GPU detectada: /physical_device:GPU:0\n",
      "✓ Crecimiento de memoria GPU: ACTIVADO\n",
      "✓ Límite de memoria GPU: 3GB\n",
      "✓ Test de operación en GPU: EXITOSO\n",
      "  Dispositivo: /job:localhost/replica:0/task:0/device:GPU:0\n",
      "✓ Soft device placement: DESACTIVADO (forzar GPU)\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 11:41:16.924601: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-07 11:41:16.968976: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-07 11:41:16.969282: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-07 11:41:16.971626: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-07 11:41:16.971880: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-07 11:41:16.972081: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-07 11:41:17.035757: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-07 11:41:17.036051: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-07 11:41:17.036210: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-07 11:41:17.036322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3072 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURACIÓN DE GPU OPTIMIZADA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
    "print(f\"¿Construido con CUDA?: {tf.test.is_built_with_cuda()}\")\n",
    "\n",
    "# Listar dispositivos físicos\n",
    "print(\"\\n--- Dispositivos disponibles ---\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "cpus = tf.config.list_physical_devices('CPU')\n",
    "print(f\"CPUs: {len(cpus)}\")\n",
    "print(f\"GPUs: {len(gpus)}\")\n",
    "\n",
    "if gpus:\n",
    "    print(f\"\\n✓ GPU detectada: {gpus[0].name}\")\n",
    "    \n",
    "    try:\n",
    "        # Configurar crecimiento de memoria (CRÍTICO para GTX 1050)\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        # Limitar memoria a 3GB\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=3072)])\n",
    "        \n",
    "        print(\"✓ Crecimiento de memoria GPU: ACTIVADO\")\n",
    "        print(\"✓ Límite de memoria GPU: 3GB\")\n",
    "        \n",
    "        # Configurar para forzar operaciones en GPU\n",
    "        tf.config.set_soft_device_placement(False)\n",
    "        \n",
    "        # Test simple en GPU\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "            b = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "        \n",
    "        print(f\"✓ Test de operación en GPU: EXITOSO\")\n",
    "        print(f\"  Dispositivo: {c.device}\")\n",
    "        print(\"✓ Soft device placement: DESACTIVADO (forzar GPU)\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"\\n⚠ Error configurando GPU: {e}\")\n",
    "else:\n",
    "    print(\"\\n⚠ No se detectaron GPUs\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037d6e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURACIÓN DE GPU\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
    "print(f\"¿Construido con CUDA?: {tf.test.is_built_with_cuda()}\")\n",
    "\n",
    "# Listar dispositivos físicos\n",
    "print(\"\\n--- Dispositivos disponibles ---\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "cpus = tf.config.list_physical_devices('CPU')\n",
    "print(f\"CPUs: {len(cpus)}\")\n",
    "print(f\"GPUs: {len(gpus)}\")\n",
    "\n",
    "if gpus:\n",
    "    print(f\"\\n✓ GPU detectada: {gpus[0].name}\")\n",
    "    \n",
    "    try:\n",
    "        # Configurar crecimiento de memoria (CRÍTICO para GTX 1050)\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        # Limitar memoria a 2.5GB (dejar 1.5GB libres para el sistema)\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=2560)])\n",
    "        \n",
    "        print(\"✓ Crecimiento de memoria GPU: ACTIVADO\")\n",
    "        print(\"✓ Límite de memoria GPU: 2.5GB (dejando margen para el sistema)\")\n",
    "        \n",
    "        # Test simple en GPU\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "            b = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "        \n",
    "        print(f\"✓ Test de operación en GPU: EXITOSO\")\n",
    "        print(f\"  Dispositivo: {c.device}\")\n",
    "        \n",
    "        # Información de GPUs lógicas\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(f\"\\nGPUs lógicas configuradas: {len(logical_gpus)}\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"\\n⚠ Error configurando GPU: {e}\")\n",
    "else:\n",
    "    print(\"\\n⚠ No se detectaron GPUs\")\n",
    "    print(\"El entrenamiento se realizará en CPU (será más lento)\")\n",
    "    print(\"\\nPara habilitar GPU, ejecuta en terminal:\")\n",
    "    print(\"  pip install tensorflow[and-cuda]==2.16.2\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6eb0b2",
   "metadata": {},
   "source": [
    "## Imporación de librerias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac4d58ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import io\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from keras.layers import Input, TimeDistributed, CategoryEncoding, SimpleRNN, Dense, GRU\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296130d1",
   "metadata": {},
   "source": [
    "## Selección del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2389627",
   "metadata": {},
   "source": [
    "Seleccione como corpus el texto de la novela Crimen y Castigo de Fiódor Mijáilovich Dostoyevski, extraído del sitio web https://www.textos.info/. El texto se encuentra en español y me resulta familiar para analizar contextos o referencias que puedan resultar de la generación de secuencias que debe realizar el modelo a entrenar, lo que facilitará el análisis de los resultados, y por otro lado es agradable trabajar con material literario que disfruté en algún momento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcc71a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del corpus: 1044667 caracteres\n",
      "Primeros 500 caracteres:\n",
      "primera parte\n",
      "i\n",
      "una tarde muy calurosa de principios de julio, salió del cuartito que ocupaba, junto al techo de una gran casa de cinco pisos, un joven, que, lentamente y con aire irresoluto, se dirigió hacia el puente de k***.\n",
      "\n",
      "tuvo suerte, al bajar la escalera, de no encontrarse a su patrona que habitaba en el piso cuarto, y cuya cocina, que tenía la puerta constantemente sin cerrar, daba a la escalera. cuando salía el joven, había de pasar forzosamente bajo el fuego del enemigo, y cada vez qu\n"
     ]
    }
   ],
   "source": [
    "# Lectura del corpus\n",
    "with open('crimen-ycastigo.txt', 'r', encoding='utf-8') as f:\n",
    "    article_text = f.read()\n",
    "\n",
    "# Pasar todo el texto a minúscula\n",
    "article_text = article_text.lower()\n",
    "\n",
    "# Verificar la longitud del corpus\n",
    "print(f\"Longitud del corpus: {len(article_text)} caracteres\")\n",
    "print(f\"Primeros 500 caracteres:\\n{article_text[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b1c429",
   "metadata": {},
   "source": [
    "## Pre-procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed385153",
   "metadata": {},
   "source": [
    "### Construcción del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "462daf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 62 caracteres únicos\n",
      "Caracteres: ['\\n', ' ', '!', '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '7', '9', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¡', '«', '»', '¿', 'á', 'ä', 'é', 'í', 'ï', 'ñ', 'ó', 'ö', 'ú', 'û', 'ü', '—']\n"
     ]
    }
   ],
   "source": [
    "# El vocabulario es el conjunto único de caracteres que existe en todo el texto\n",
    "chars_vocab = set(article_text)\n",
    "\n",
    "# La longitud del vocabulario de caracteres es:\n",
    "vocab_size = len(chars_vocab)\n",
    "print(f\"Tamaño del vocabulario: {vocab_size} caracteres únicos\")\n",
    "print(f\"Caracteres: {sorted(chars_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d146e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo los diccionarios que asignan índices a caracteres y viceversa\n",
    "# El diccionario char2idx servirá como tokenizador\n",
    "char2idx = {k: v for v, k in enumerate(chars_vocab)}\n",
    "idx2char = {v: k for k, v in char2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ade1738",
   "metadata": {},
   "source": [
    "Visualización de los diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e9789f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char to Index:\n",
      "{'d': 0, ')': 1, 'á': 2, 'û': 3, 'ñ': 4, '1': 5, 'y': 6, 'f': 7, 'q': 8, 'ü': 9, '2': 10, ':': 11, '—': 12, '!': 13, '3': 14, '?': 15, 'i': 16, 'ä': 17, 'g': 18, 'k': 19, 'w': 20, 'x': 21, 'n': 22, 'j': 23, 'h': 24, '(': 25, 'ö': 26, 'l': 27, 'p': 28, '4': 29, 'u': 30, 'ó': 31, 'ú': 32, 't': 33, 'z': 34, 'o': 35, '¡': 36, '7': 37, 'e': 38, '9': 39, ',': 40, ';': 41, '*': 42, '\\n': 43, '0': 44, '«': 45, '.': 46, 'b': 47, '5': 48, 'ï': 49, 'a': 50, 'í': 51, 's': 52, '¿': 53, ' ': 54, 'é': 55, '-': 56, 'c': 57, 'v': 58, '»': 59, 'm': 60, 'r': 61}\n",
      "Index to Char:\n",
      "{0: 'd', 1: ')', 2: 'á', 3: 'û', 4: 'ñ', 5: '1', 6: 'y', 7: 'f', 8: 'q', 9: 'ü', 10: '2', 11: ':', 12: '—', 13: '!', 14: '3', 15: '?', 16: 'i', 17: 'ä', 18: 'g', 19: 'k', 20: 'w', 21: 'x', 22: 'n', 23: 'j', 24: 'h', 25: '(', 26: 'ö', 27: 'l', 28: 'p', 29: '4', 30: 'u', 31: 'ó', 32: 'ú', 33: 't', 34: 'z', 35: 'o', 36: '¡', 37: '7', 38: 'e', 39: '9', 40: ',', 41: ';', 42: '*', 43: '\\n', 44: '0', 45: '«', 46: '.', 47: 'b', 48: '5', 49: 'ï', 50: 'a', 51: 'í', 52: 's', 53: '¿', 54: ' ', 55: 'é', 56: '-', 57: 'c', 58: 'v', 59: '»', 60: 'm', 61: 'r'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Char to Index:\")\n",
    "print(char2idx)\n",
    "print(\"Index to Char:\")\n",
    "print(idx2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73f5358",
   "metadata": {},
   "source": [
    "### Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9286ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del texto tokenizado: 1044667\n",
      "Primeros 100 tokens: [28, 61, 16, 60, 38, 61, 50, 54, 28, 50, 61, 33, 38, 43, 16, 43, 30, 22, 50, 54, 33, 50, 61, 0, 38, 54, 60, 30, 6, 54, 57, 50, 27, 30, 61, 35, 52, 50, 54, 0, 38, 54, 28, 61, 16, 22, 57, 16, 28, 16, 35, 52, 54, 0, 38, 54, 23, 30, 27, 16, 35, 40, 54, 52, 50, 27, 16, 31, 54, 0, 38, 27, 54, 57, 30, 50, 61, 33, 16, 33, 35, 54, 8, 30, 38, 54, 35, 57, 30, 28, 50, 47, 50, 40, 54, 23, 30, 22, 33, 35]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = [char2idx[ch] for ch in article_text]\n",
    "print(f\"Longitud del texto tokenizado: {len(tokenized_text)}\")\n",
    "print(f\"Primeros 100 tokens: {tokenized_text[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9efd14",
   "metadata": {},
   "source": [
    "### Selección del tamaño de contexto\n",
    "\n",
    "Como la tokenización es por caracter, la cantodad de tokens es igual a la cantidad de caracteres en el texto: 1044667. Todo el corpus puede ser considerado un documento  y el tamaño de contexto puede ser elegido con mayor libertad en comparación a un modelo tokenizado por palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2439dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos el tamaño de contexto\n",
    "max_context_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad31a6",
   "metadata": {},
   "source": [
    "### Estructuración del dataset\n",
    "\n",
    "Separo el dataset en entrenamiento y validación. Utilizo una 10% para validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "891ac854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de secuencias para validación: 1045\n"
     ]
    }
   ],
   "source": [
    "#p_val es la proporcion del corpus que uso para validación \n",
    "p_val = 0.1\n",
    "\n",
    "# num_val es la cantidad de secuencias de tamaño max_context_size que se usará en validación\n",
    "num_val = int(np.ceil(len(tokenized_text) * p_val / max_context_size))\n",
    "\n",
    "print(f\"Número de secuencias para validación: {num_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db83afe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del texto de entrenamiento: 940167\n",
      "Longitud del texto de validación: 104500\n"
     ]
    }
   ],
   "source": [
    "# Datos de entrenamiento\n",
    "train_text = tokenized_text[:-num_val * max_context_size]\n",
    "# Datos de validación\n",
    "val_text = tokenized_text[-num_val * max_context_size:]\n",
    "\n",
    "print(f\"Longitud del texto de entrenamiento: {len(train_text)}\")\n",
    "print(f\"Longitud del texto de validación: {len(val_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc567907",
   "metadata": {},
   "source": [
    "Creación de las secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e8370d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de secuencias de entrenamiento: 940068\n",
      "Número de secuencias de validación: 1045\n"
     ]
    }
   ],
   "source": [
    "# Secuencias de validación\n",
    "tokenized_sentences_val = [val_text[init * max_context_size:init * (max_context_size + 1)] \n",
    "                           for init in range(num_val)]\n",
    "\n",
    "# Secuencias de entrenamiento\n",
    "tokenized_sentences_train = [train_text[init:init + max_context_size] \n",
    "                             for init in range(len(train_text) - max_context_size + 1)]\n",
    "\n",
    "print(f\"Número de secuencias de entrenamiento: {len(tokenized_sentences_train)}\")\n",
    "print(f\"Número de secuencias de validación: {len(tokenized_sentences_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03bc22c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de X: (940067, 100)\n",
      "Shape de y: (940067, 100)\n",
      "\n",
      "Ejemplo - Primeros 10 tokens de X[0]: [28 61 16 60 38 61 50 54 28 50]\n",
      "Ejemplo - Primeros 10 tokens de y[0]: [61 16 60 38 61 50 54 28 50 61]\n"
     ]
    }
   ],
   "source": [
    "# Estructuramos el problema como many-to-many:\n",
    "# Entrada: secuencia de tokens [x_0, x_1, ..., x_N]\n",
    "# Target: secuencia de tokens [x_1, x_2, ..., x_{N+1}]\n",
    "\n",
    "X = np.array(tokenized_sentences_train[:-1])\n",
    "y = np.array(tokenized_sentences_train[1:])\n",
    "\n",
    "print(f\"Shape de X: {X.shape}\")\n",
    "print(f\"Shape de y: {y.shape}\")\n",
    "print(f\"\\nEjemplo - Primeros 10 tokens de X[0]: {X[0, :10]}\")\n",
    "print(f\"Ejemplo - Primeros 10 tokens de y[0]: {y[0, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff34250",
   "metadata": {},
   "source": [
    "## Arquitecturas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86049c26",
   "metadata": {},
   "source": [
    "### Callback de Perplejidad\n",
    "\n",
    "Implemento el callback ad-hoc para calcular la perplejidad al final de cada epoch de entrenamiento sobre un conjunto de datos de validación. La perplejidad es una métrica cuantitativa para evaluar la calidad de la generación de secuencias. Además implementa Early Stopping si la perplejidad no mejora después de `patience` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71bedfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PplCallback(keras.callbacks.Callback):\n",
    "    '''\n",
    "    Este callback es una solución ad-hoc para calcular al final de cada epoch de\n",
    "    entrenamiento la métrica de Perplejidad sobre un conjunto de datos de validación.\n",
    "    La perplejidad es una métrica cuantitativa para evaluar la calidad de la generación de secuencias.\n",
    "    Además implementa la finalización del entrenamiento (Early Stopping)\n",
    "    si la perplejidad no mejora después de `patience` epochs.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, val_data, history_ppl, patience=5, batch_size=64):\n",
    "        # El callback lo inicializamos con secuencias de validación sobre las cuales\n",
    "        # mediremos la perplejidad\n",
    "        self.val_data = val_data\n",
    "        self.batch_size = batch_size  # Batch size para predicciones (menor para evitar OOM)\n",
    "\n",
    "        self.target = []\n",
    "        self.padded = []\n",
    "\n",
    "        count = 0\n",
    "        self.info = []\n",
    "        self.min_score = np.inf\n",
    "        self.patience_counter = 0\n",
    "        self.patience = patience\n",
    "\n",
    "        # nos movemos en todas las secuencias de los datos de validación\n",
    "        for seq in self.val_data:\n",
    "            len_seq = len(seq)\n",
    "            # armamos todas las subsecuencias\n",
    "            subseq = [seq[:i] for i in range(1, len_seq)]\n",
    "            self.target.extend([seq[i] for i in range(1, len_seq)])\n",
    "\n",
    "            if len(subseq) != 0:\n",
    "                self.padded.append(pad_sequences(subseq, maxlen=max_context_size, padding='pre'))\n",
    "                self.info.append((count, count + len_seq))\n",
    "                count += len_seq\n",
    "\n",
    "        self.padded = np.vstack(self.padded)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # en `scores` iremos guardando la perplejidad de cada secuencia\n",
    "        scores = []\n",
    "\n",
    "        # Predicciones en batches pequeños para evitar OOM\n",
    "        predictions = []\n",
    "        num_samples = len(self.padded)\n",
    "        \n",
    "        for i in range(0, num_samples, self.batch_size):\n",
    "            batch = self.padded[i:i + self.batch_size]\n",
    "            batch_pred = self.model.predict(batch, verbose=0)\n",
    "            predictions.append(batch_pred)\n",
    "        \n",
    "        # Concatenar todas las predicciones\n",
    "        predictions = np.vstack(predictions)\n",
    "\n",
    "        # para cada secuencia de validación\n",
    "        for start, end in self.info:\n",
    "            # en `probs` iremos guardando las probabilidades de los términos target\n",
    "            probs = [predictions[idx_seq, -1, idx_vocab] \n",
    "                    for idx_seq, idx_vocab in zip(range(start, end), self.target[start:end])]\n",
    "\n",
    "            # calculamos la perplejidad por medio de logaritmos\n",
    "            scores.append(np.exp(-np.sum(np.log(probs + 1E-10)) / (end - start)))\n",
    "\n",
    "        # promediamos todos los scores e imprimimos el valor promedio\n",
    "        current_score = np.mean(scores)\n",
    "        history_ppl.append(current_score)\n",
    "        print(f'\\nmean perplexity: {current_score}\\n')\n",
    "\n",
    "        # chequeamos si tenemos que detener el entrenamiento\n",
    "        if current_score < self.min_score:\n",
    "            self.min_score = current_score\n",
    "            self.model.save(\"best_model.keras\")\n",
    "            print(\"Saved new model!\")\n",
    "            self.patience_counter = 0\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "            if self.patience_counter == self.patience:\n",
    "                print(\"Stopping training...\")\n",
    "                self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4dd1a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear diferentes arquitecturas de modelos\n",
    "def crear_modelo(tipo_rnn='SimpleRNN', hidden_units=200, dropout=0.1, recurrent_dropout=0.1):\n",
    "    \"\"\"\n",
    "    Crea un modelo de lenguaje con la arquitectura especificada.\n",
    "    \n",
    "    Args:\n",
    "        tipo_rnn: tipo de capa recurrente ('SimpleRNN', 'LSTM', 'GRU')\n",
    "        hidden_units: número de unidades ocultas en la capa recurrente\n",
    "        dropout: tasa de dropout\n",
    "        recurrent_dropout: tasa de dropout recurrente\n",
    "    \n",
    "    Returns:\n",
    "        modelo compilado\n",
    "    \"\"\"\n",
    "    with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        # Capa de codificación one-hot\n",
    "        model.add(TimeDistributed(CategoryEncoding(num_tokens=vocab_size, output_mode=\"one_hot\"),\n",
    "                                input_shape=(None, 1)))\n",
    "        \n",
    "        # Capa recurrente según el tipo especificado\n",
    "        if tipo_rnn == 'SimpleRNN':\n",
    "            model.add(SimpleRNN(hidden_units, return_sequences=True, \n",
    "                            dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "        elif tipo_rnn == 'LSTM':\n",
    "            model.add(LSTM(hidden_units, return_sequences=True, \n",
    "                        dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "        elif tipo_rnn == 'GRU':\n",
    "            model.add(GRU(hidden_units, return_sequences=True, \n",
    "                        dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "        else:\n",
    "            raise ValueError(f\"Tipo de RNN no soportado: {tipo_rnn}\")\n",
    "        \n",
    "        # Capa de salida\n",
    "        model.add(Dense(vocab_size, activation='softmax'))\n",
    "        \n",
    "        # Compilación del modelo\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff23d7",
   "metadata": {},
   "source": [
    "#### Modelo 1: SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6b093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gury/python/pln/lib/python3.12/site-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">52,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,462</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)      │        \u001b[38;5;34m52,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)       │        \u001b[38;5;34m12,462\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">65,062</span> (254.15 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m65,062\u001b[0m (254.15 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">65,062</span> (254.15 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m65,062\u001b[0m (254.15 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear modelo SimpleRNN\n",
    "model_simplernn = crear_modelo(tipo_rnn='SimpleRNN', hidden_units=200)\n",
    "model_simplernn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e5851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset: 940067 secuencias\n",
      "Memoria aproximada: 0.70 GB\n",
      "\n",
      "✓ GPU disponible y funcional para entrenamiento\n",
      "Iniciando entrenamiento con batch_size=64\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765118778.417285  143639 service.cc:145] XLA service 0x7dab50006a70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1765118778.417350  143639 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce GTX 1050, Compute Capability 6.1\n",
      "2025-12-07 11:46:18.803886: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "W0000 00:00:1765118778.838716  143639 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-12-07 11:46:18.926066: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "2025-12-07 11:46:18.803886: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "W0000 00:00:1765118778.838716  143639 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-12-07 11:46:18.926066: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "2025-12-07 11:46:22.446832: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:742] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "2025-12-07 11:46:22.446832: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:742] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   13/14689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:23\u001b[0m 14ms/step - loss: 3.6619"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765118783.116371  143639 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14686/14689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.9715"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1765118987.742688  143635 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14689/14689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.9714"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo SimpleRNN\n",
    "# Usar batch_size más pequeño para evitar problemas de memoria\n",
    "history_ppl_simplernn = []\n",
    "hist_simplernn = model_simplernn.fit(X, y, epochs=20, \n",
    "                                      callbacks=[PplCallback(tokenized_sentences_val, history_ppl_simplernn, patience=5, batch_size=32)], \n",
    "                                      batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd243df3",
   "metadata": {},
   "source": [
    "#### Modelo 2: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d1f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelo LSTM\n",
    "model_lstm = crear_modelo(tipo_rnn='LSTM', hidden_units=200)\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb188649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo LSTM\n",
    "history_ppl_lstm = []\n",
    "hist_lstm = model_lstm.fit(X, y, epochs=20, \n",
    "                            callbacks=[PplCallback(tokenized_sentences_val, history_ppl_lstm, patience=5, batch_size=32)], \n",
    "                            batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b65831",
   "metadata": {},
   "source": [
    "#### Modelo 3: GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d545c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelo GRU\n",
    "model_gru = crear_modelo(tipo_rnn='GRU', hidden_units=200)\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826a7f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo GRU\n",
    "history_ppl_gru = []\n",
    "hist_gru = model_gru.fit(X, y, epochs=20, \n",
    "                         callbacks=[PplCallback(tokenized_sentences_val, history_ppl_gru, patience=5, batch_size=32)], \n",
    "                         batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf96e8",
   "metadata": {},
   "source": [
    "### Comparación de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344aa849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualización de la perplejidad durante el entrenamiento\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# SimpleRNN\n",
    "if len(history_ppl_simplernn) > 0:\n",
    "    epoch_count = range(1, len(history_ppl_simplernn) + 1)\n",
    "    axes[0].plot(epoch_count, history_ppl_simplernn, marker='o')\n",
    "    axes[0].set_title('SimpleRNN - Perplejidad en Validación')\n",
    "    axes[0].set_xlabel('Época')\n",
    "    axes[0].set_ylabel('Perplejidad')\n",
    "    axes[0].grid(True)\n",
    "\n",
    "# LSTM\n",
    "if len(history_ppl_lstm) > 0:\n",
    "    epoch_count = range(1, len(history_ppl_lstm) + 1)\n",
    "    axes[1].plot(epoch_count, history_ppl_lstm, marker='o', color='orange')\n",
    "    axes[1].set_title('LSTM - Perplejidad en Validación')\n",
    "    axes[1].set_xlabel('Época')\n",
    "    axes[1].set_ylabel('Perplejidad')\n",
    "    axes[1].grid(True)\n",
    "\n",
    "# GRU\n",
    "if len(history_ppl_gru) > 0:\n",
    "    epoch_count = range(1, len(history_ppl_gru) + 1)\n",
    "    axes[2].plot(epoch_count, history_ppl_gru, marker='o', color='green')\n",
    "    axes[2].set_title('GRU - Perplejidad en Validación')\n",
    "    axes[2].set_xlabel('Época')\n",
    "    axes[2].set_ylabel('Perplejidad')\n",
    "    axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparación de perplejidad mínima\n",
    "print(\"\\n=== Resumen de Resultados ===\")\n",
    "if len(history_ppl_simplernn) > 0:\n",
    "    print(f\"SimpleRNN - Perplejidad mínima: {min(history_ppl_simplernn):.4f}\")\n",
    "if len(history_ppl_lstm) > 0:\n",
    "    print(f\"LSTM - Perplejidad mínima: {min(history_ppl_lstm):.4f}\")\n",
    "if len(history_ppl_gru) > 0:\n",
    "    print(f\"GRU - Perplejidad mínima: {min(history_ppl_gru):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb783243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el mejor modelo guardado para hacer inferencia\n",
    "best_model = keras.models.load_model('best_model.keras')\n",
    "print(\"Mejor modelo cargado correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5511e5f",
   "metadata": {},
   "source": [
    "## Generación de secuencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b0ea2",
   "metadata": {},
   "source": [
    "### Funciones auxiliares de codificación y decodificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcionalidades para hacer encoding y decoding\n",
    "\n",
    "def encode(text, max_length=max_context_size):\n",
    "    \"\"\"Codifica texto en secuencia de índices\"\"\"\n",
    "    encoded = [char2idx[ch] for ch in text.lower()]\n",
    "    encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "    return encoded\n",
    "\n",
    "def decode(seq):\n",
    "    \"\"\"Decodifica secuencia de índices a texto\"\"\"\n",
    "    return ''.join([idx2char[ch] for ch in seq])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e77fdb",
   "metadata": {},
   "source": [
    "### 1. Greedy Search\n",
    "\n",
    "La estrategia más simple: en cada paso seleccionar el carácter con mayor probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e547f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq_greedy(model, seed_text, max_length, n_chars):\n",
    "    \"\"\"\n",
    "    Generación de secuencias con greedy search.\n",
    "    \n",
    "    Args:\n",
    "        model: modelo entrenado\n",
    "        seed_text: texto de entrada (contexto inicial)\n",
    "        max_length: máxima longitud de la secuencia de entrada\n",
    "        n_chars: números de caracteres a generar\n",
    "        \n",
    "    Returns:\n",
    "        output_text: texto generado\n",
    "    \"\"\"\n",
    "    output_text = seed_text\n",
    "    \n",
    "    # Generar un número fijo de caracteres\n",
    "    for _ in range(n_chars):\n",
    "        # Codificamos\n",
    "        encoded = [char2idx[ch] for ch in output_text.lower()]\n",
    "        # Padding si es necesario\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "        \n",
    "        # Predicción - seleccionamos el carácter con mayor probabilidad\n",
    "        y_hat = np.argmax(model.predict(encoded, verbose=0)[0, -1, :])\n",
    "        \n",
    "        # Decodificamos y agregamos al texto\n",
    "        out_char = idx2char[y_hat]\n",
    "        output_text += out_char\n",
    "        \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d9ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba de greedy search con diferentes contextos\n",
    "input_texts = [\n",
    "    'una tarde',\n",
    "    'raskolnikoff',\n",
    "    'san petersburgo',\n",
    "    'la vieja'\n",
    "]\n",
    "\n",
    "print(\"=== GENERACIÓN CON GREEDY SEARCH ===\\n\")\n",
    "for input_text in input_texts:\n",
    "    generated = generate_seq_greedy(best_model, input_text, max_length=max_context_size, n_chars=100)\n",
    "    print(f\"Contexto: '{input_text}'\")\n",
    "    print(f\"Generado: {generated}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd98ac0a",
   "metadata": {},
   "source": [
    "### 2. Beam Search\n",
    "\n",
    "Implementación de beam search determinístico y estocástico con temperatura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd92812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "def select_candidates(pred, num_beams, vocab_size, history_probs, history_tokens, temp, mode):\n",
    "    \"\"\"\n",
    "    Selecciona candidatos para el beam search.\n",
    "    \n",
    "    Args:\n",
    "        pred: predicciones del modelo\n",
    "        num_beams: número de beams a mantener\n",
    "        vocab_size: tamaño del vocabulario\n",
    "        history_probs: historial de probabilidades logarítmicas\n",
    "        history_tokens: historial de tokens\n",
    "        temp: temperatura para muestreo estocástico\n",
    "        mode: 'det' para determinístico, 'sto' para estocástico\n",
    "        \n",
    "    Returns:\n",
    "        nuevas probabilidades e historial de tokens\n",
    "    \"\"\"\n",
    "    # Colectar todas las probabilidades para la siguiente búsqueda\n",
    "    pred_large = []\n",
    "    \n",
    "    for idx, pp in enumerate(pred):\n",
    "        pred_large.extend(np.log(pp + 1E-10) + history_probs[idx])\n",
    "    \n",
    "    pred_large = np.array(pred_large)\n",
    "    \n",
    "    # Criterio de selección\n",
    "    if mode == 'det':\n",
    "        # Beam search determinístico\n",
    "        idx_select = np.argsort(pred_large)[::-1][:num_beams]\n",
    "    elif mode == 'sto':\n",
    "        # Beam search con muestreo aleatorio\n",
    "        idx_select = np.random.choice(np.arange(pred_large.shape[0]), \n",
    "                                     num_beams, \n",
    "                                     p=softmax(pred_large / temp))\n",
    "    else:\n",
    "        raise ValueError(f'Modo de selección incorrecto. {mode} fue dado. \"det\" y \"sto\" son soportados.')\n",
    "    \n",
    "    # Traducir a índices de token en el vocabulario\n",
    "    new_history_tokens = np.concatenate((np.array(history_tokens)[idx_select // vocab_size],\n",
    "                                        np.array([idx_select % vocab_size]).T),\n",
    "                                       axis=1)\n",
    "    \n",
    "    # Devolver el producto de las probabilidades (log) y la secuencia de tokens seleccionados\n",
    "    return pred_large[idx_select.astype(int)], new_history_tokens.astype(int)\n",
    "\n",
    "\n",
    "def beam_search(model, num_beams, num_chars, input_text, temp=1, mode='det'):\n",
    "    \"\"\"\n",
    "    Implementa beam search para generación de texto.\n",
    "    \n",
    "    Args:\n",
    "        model: modelo entrenado\n",
    "        num_beams: número de beams a mantener\n",
    "        num_chars: número de caracteres a generar\n",
    "        input_text: texto de contexto inicial\n",
    "        temp: temperatura (solo para modo estocástico)\n",
    "        mode: 'det' para determinístico, 'sto' para estocástico\n",
    "        \n",
    "    Returns:\n",
    "        secuencias generadas\n",
    "    \"\"\"\n",
    "    # Primera iteración\n",
    "    \n",
    "    # Codificar\n",
    "    encoded = encode(input_text)\n",
    "    \n",
    "    # Primera predicción\n",
    "    y_hat = model.predict(encoded, verbose=0)[0, -1, :]\n",
    "    \n",
    "    # Obtener tamaño del vocabulario\n",
    "    vocab_size_local = y_hat.shape[0]\n",
    "    \n",
    "    # Inicializar historial\n",
    "    history_probs = [0] * num_beams\n",
    "    history_tokens = [encoded[0]] * num_beams\n",
    "    \n",
    "    # Seleccionar num_beams candidatos\n",
    "    history_probs, history_tokens = select_candidates([y_hat],\n",
    "                                                      num_beams,\n",
    "                                                      vocab_size_local,\n",
    "                                                      history_probs,\n",
    "                                                      history_tokens,\n",
    "                                                      temp,\n",
    "                                                      mode)\n",
    "    \n",
    "    # Loop de beam search\n",
    "    for i in range(num_chars - 1):\n",
    "        preds = []\n",
    "        \n",
    "        for hist in history_tokens:\n",
    "            # Actualizar secuencia de tokens\n",
    "            input_update = np.array([hist[i + 1:]]).copy()\n",
    "            \n",
    "            # Predicción\n",
    "            y_hat = model.predict(input_update, verbose=0)[0, -1, :]\n",
    "            \n",
    "            preds.append(y_hat)\n",
    "        \n",
    "        history_probs, history_tokens = select_candidates(preds,\n",
    "                                                          num_beams,\n",
    "                                                          vocab_size_local,\n",
    "                                                          history_probs,\n",
    "                                                          history_tokens,\n",
    "                                                          temp,\n",
    "                                                          mode)\n",
    "    \n",
    "    return history_tokens[:, -(len(input_text) + num_chars):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b02ac1",
   "metadata": {},
   "source": [
    "### 3. Beam Search Determinístico\n",
    "\n",
    "Prueba de beam search determinístico con diferentes números de beams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919712f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear diferentes arquitecturas de modelos (OPTIMIZADA PARA GPU)\n",
    "def crear_modelo(tipo_rnn='SimpleRNN', hidden_units=200, dropout=0.1, recurrent_dropout=0.1, embedding_dim=64):\n",
    "    \"\"\"\n",
    "    Crea un modelo de lenguaje con la arquitectura especificada.\n",
    "    Optimizado para GPU usando Embedding en lugar de One-Hot Encoding.\n",
    "    \n",
    "    Args:\n",
    "        tipo_rnn: tipo de capa recurrente ('SimpleRNN', 'LSTM', 'GRU')\n",
    "        hidden_units: número de unidades ocultas en la capa recurrente\n",
    "        dropout: tasa de dropout\n",
    "        recurrent_dropout: tasa de dropout recurrente\n",
    "        embedding_dim: dimensión del embedding (más eficiente que OHE)\n",
    "    \n",
    "    Returns:\n",
    "        modelo compilado\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Usar Embedding en lugar de One-Hot Encoding (mucho más eficiente en GPU)\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, \n",
    "                        input_length=max_context_size))\n",
    "    \n",
    "    # Capa recurrente según el tipo especificado\n",
    "    if tipo_rnn == 'SimpleRNN':\n",
    "        model.add(SimpleRNN(hidden_units, return_sequences=True, \n",
    "                           dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "    elif tipo_rnn == 'LSTM':\n",
    "        model.add(LSTM(hidden_units, return_sequences=True, \n",
    "                      dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "    elif tipo_rnn == 'GRU':\n",
    "        model.add(GRU(hidden_units, return_sequences=True, \n",
    "                     dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "    else:\n",
    "        raise ValueError(f\"Tipo de RNN no soportado: {tipo_rnn}\")\n",
    "    \n",
    "    # Capa de salida\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    \n",
    "    # Compilación del modelo con optimizador Adam (más estable en GPU)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', \n",
    "                 optimizer='adam',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb30bf",
   "metadata": {},
   "source": [
    "### 4. Beam Search Estocástico con Temperatura\n",
    "\n",
    "Prueba de beam search estocástico explorando el efecto de la temperatura. \n",
    "- **Temperatura baja** (< 1): hace la distribución más \"determinística\", favoreciendo tokens con alta probabilidad\n",
    "- **Temperatura alta** (> 1): hace la distribución más uniforme, favoreciendo diversidad y exploración\n",
    "- **Temperatura = 1**: distribución original sin modificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0ba3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba de beam search estocástico con diferentes temperaturas\n",
    "input_text = \"raskolnikoff\"\n",
    "num_beams = 5\n",
    "num_chars_to_generate = 100\n",
    "\n",
    "print(\"=== BEAM SEARCH ESTOCÁSTICO - EFECTO DE LA TEMPERATURA ===\\n\")\n",
    "print(f\"Contexto inicial: '{input_text}'\")\n",
    "print(f\"Número de beams: {num_beams}\")\n",
    "print(f\"Caracteres a generar: {num_chars_to_generate}\\n\")\n",
    "\n",
    "temperaturas = [0.3, 0.7, 1.0, 1.5, 2.0]\n",
    "\n",
    "for temp in temperaturas:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TEMPERATURA = {temp}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    salidas = beam_search(best_model, num_beams=num_beams, num_chars=num_chars_to_generate, \n",
    "                         input_text=input_text, temp=temp, mode='sto')\n",
    "    \n",
    "    for idx, salida in enumerate(salidas[:3], 1):  # Mostrar las top 3\n",
    "        texto_generado = decode(salida)\n",
    "        print(f\"{idx}. {texto_generado}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96211bf3",
   "metadata": {},
   "source": [
    "# Entrenamiento del modelo SimpleRNN\n",
    "# IMPORTANTE: Reducir dataset si hay problemas de memoria\n",
    "print(f\"Tamaño del dataset completo: {len(X)} secuencias\")\n",
    "print(f\"Memoria aproximada: {X.nbytes / (1024**3):.2f} GB\")\n",
    "\n",
    "# Usar un subconjunto del dataset si es necesario (descomentar si sigue habiendo problemas)\n",
    "# X_train = X[:100000]  # Usar solo 100k secuencias\n",
    "# y_train = y[:100000]\n",
    "\n",
    "# Entrenamiento con batch pequeño para GTX 1050\n",
    "history_ppl_simplernn = []\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    hist_simplernn = model_simplernn.fit(\n",
    "        X, y, \n",
    "        epochs=20, \n",
    "        callbacks=[PplCallback(tokenized_sentences_val, history_ppl_simplernn, patience=5, batch_size=32)], \n",
    "        batch_size=64,  # Batch pequeño para GPU de 4GB\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11479ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación de estrategias con el mismo contexto\n",
    "input_text = \"san petersburgo\"\n",
    "n_chars = 150\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"COMPARACIÓN DE ESTRATEGIAS DE GENERACIÓN\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nContexto inicial: '{input_text}'\")\n",
    "print(f\"Caracteres a generar: {n_chars}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"1. GREEDY SEARCH\")\n",
    "print(\"=\"*100)\n",
    "resultado_greedy = generate_seq_greedy(best_model, input_text, max_context_size, n_chars)\n",
    "print(resultado_greedy)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"2. BEAM SEARCH DETERMINÍSTICO (10 beams)\")\n",
    "print(\"=\"*100)\n",
    "salidas_det = beam_search(best_model, num_beams=10, num_chars=n_chars, \n",
    "                         input_text=input_text, mode='det')\n",
    "print(decode(salidas_det[0]))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"3. BEAM SEARCH ESTOCÁSTICO (10 beams, temperatura=0.7)\")\n",
    "print(\"=\"*100)\n",
    "salidas_sto_07 = beam_search(best_model, num_beams=10, num_chars=n_chars, \n",
    "                             input_text=input_text, temp=0.7, mode='sto')\n",
    "print(decode(salidas_sto_07[0]))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"4. BEAM SEARCH ESTOCÁSTICO (10 beams, temperatura=1.5)\")\n",
    "print(\"=\"*100)\n",
    "salidas_sto_15 = beam_search(best_model, num_beams=10, num_chars=n_chars, \n",
    "                             input_text=input_text, temp=1.5, mode='sto')\n",
    "print(decode(salidas_sto_15[0]))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb1232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelo LSTM\n",
    "model_lstm = crear_modelo(tipo_rnn='LSTM', hidden_units=200, embedding_dim=64)\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f09b1",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "### Sobre el corpus y pre-procesamiento\n",
    "- Se utilizó el texto completo de \"Crimen y Castigo\" de Dostoyevski en español, con aproximadamente 500,000 caracteres.\n",
    "- El corpus fue tokenizado a nivel de caracteres, creando un vocabulario de aproximadamente 60-80 caracteres únicos.\n",
    "- Se utilizó una ventana de contexto de 100 caracteres para capturar dependencias de largo alcance.\n",
    "- Los datos se dividieron en 90% entrenamiento y 10% validación.\n",
    "\n",
    "### Sobre las arquitecturas\n",
    "Se probaron tres arquitecturas de redes recurrentes:\n",
    "1. **SimpleRNN**: Arquitectura básica, más simple pero con limitaciones para capturar dependencias largas.\n",
    "2. **LSTM**: Maneja mejor las dependencias de largo plazo gracias a sus puertas de olvido y memoria.\n",
    "3. **GRU**: Similar a LSTM pero con menos parámetros, buen balance entre eficiencia y capacidad.\n",
    "\n",
    "La perplejidad en validación permitió seleccionar el mejor modelo mediante early stopping.\n",
    "\n",
    "### Sobre las estrategias de generación\n",
    "\n",
    "1. **Greedy Search**:\n",
    "   - Estrategia más simple: selecciona siempre el carácter con mayor probabilidad.\n",
    "   - Genera texto coherente pero puede caer en repeticiones.\n",
    "   - Computacionalmente eficiente.\n",
    "\n",
    "2. **Beam Search Determinístico**:\n",
    "   - Mantiene múltiples hipótesis (beams) simultáneamente.\n",
    "   - Explora más el espacio de búsqueda que greedy search.\n",
    "   - Mayor número de beams → mayor calidad pero mayor costo computacional.\n",
    "\n",
    "3. **Beam Search Estocástico con Temperatura**:\n",
    "   - **Temperatura baja (0.3-0.7)**: Texto más conservador y coherente, favorece tokens probables.\n",
    "   - **Temperatura media (1.0)**: Balance entre coherencia y diversidad.\n",
    "   - **Temperatura alta (1.5-2.0)**: Mayor creatividad y diversidad, pero potencialmente menos coherente.\n",
    "\n",
    "### Observaciones finales\n",
    "- El modelo de lenguaje por caracteres permite generar texto fluido en español.\n",
    "- La temperatura es un hiperparámetro crucial para controlar el balance creatividad/coherencia.\n",
    "- Las arquitecturas LSTM y GRU generalmente superan a SimpleRNN en la captura de contexto.\n",
    "- El corpus literario permite generar texto con estilo similar a la novela original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c76a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo GRU\n",
    "history_ppl_gru = []\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    hist_gru = model_gru.fit(\n",
    "        X, y, \n",
    "        epochs=20, \n",
    "        callbacks=[PplCallback(tokenized_sentences_val, history_ppl_gru, patience=5, batch_size=32)], \n",
    "        batch_size=64,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7139f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelo GRU\n",
    "model_gru = crear_modelo(tipo_rnn='GRU', hidden_units=200, embedding_dim=64)\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea80f1",
   "metadata": {},
   "source": [
    "#### Modelo 3: GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81582301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo LSTM\n",
    "history_ppl_lstm = []\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    hist_lstm = model_lstm.fit(\n",
    "        X, y, \n",
    "        epochs=20, \n",
    "        callbacks=[PplCallback(tokenized_sentences_val, history_ppl_lstm, patience=5, batch_size=32)], \n",
    "        batch_size=64,\n",
    "        verbose=1\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLN (Python 3.12.3)",
   "language": "python",
   "name": "pln"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
